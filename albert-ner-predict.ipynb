{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用`AlBert`进行命名实体识别\n",
    "***\n",
    "***\n",
    "Time: 2020-09-21\n",
    "Author: dsy\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模块库导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as tud # 自定义数据集 \n",
    "import math\n",
    "import pandas as pd\n",
    "from model.torchcrf import CRF\n",
    "import os\n",
    "from model.albert_pytorch.modeling_albert import AlbertConfig, AlbertForPreTraining,AlbertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "常量定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model/albert_pytorch/prev_trained_model/albert_base_v2/config.json'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_length = 202\n",
    "BATCH_SIZE = 8\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n",
    "config_file = \"model/albert_pytorch/prev_trained_model/albert_base_v2/config.json\"\n",
    "config_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建vocab\n",
    "def readVocab(filepath:str=\"data/vocab.txt\"):\n",
    "    vocab = []\n",
    "    with open(filepath,\"r\",encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            vocab.append(line.strip())\n",
    "            \n",
    "    with open(\"data/train.txt\",\"r\",encoding=\"utf-8\") as fp:\n",
    "        for line in fp:\n",
    "            if '' != line.strip():\n",
    "                vocab.append(line.strip().split(\" \")[0])\n",
    "            \n",
    "    vocab = set(vocab)        \n",
    "    vocab2id = { data:1+i for i,data in enumerate(vocab)}\n",
    "    id2vocab = {1+i:data for i,data in enumerate(vocab)}\n",
    "    return (vocab,vocab2id,id2vocab,len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle(filepath:str=\"data/train.txt\"):\n",
    "    words = []\n",
    "    labels = []\n",
    "    vocab = []\n",
    "    label_list = set()\n",
    "    label_list.add(\"[CLS]\")\n",
    "    label_list.add(\"[SEP]\")\n",
    "    with open(filepath,\"r\",encoding=\"utf-8\") as fp:\n",
    "        word = []\n",
    "        label = []\n",
    "        for index,line in enumerate( fp):\n",
    "            \n",
    "            if '' == line.strip():\n",
    "                words.append(word)\n",
    "                labels.append(label)\n",
    "#                 print(\"words:\\n\",pd.DataFrame(words))\n",
    "#                 print(\"labels:\\n\",pd.DataFrame(labels))\n",
    "                word = []\n",
    "                label = []\n",
    "            else:\n",
    "                linesplit = line.strip().split(\" \")\n",
    "                word.append(linesplit[0])\n",
    "                label.append(linesplit[1])\n",
    "                label_list.add(linesplit[1])\n",
    "                \n",
    "    label2id = {data:i+1 for i,data in enumerate(label_list)}\n",
    "    id2label = {1+i:data for i,data in enumerate(label_list)}\n",
    "    \n",
    "    input_ids = []\n",
    "    for i in words:\n",
    "        input_id = []\n",
    "    #     break\n",
    "        for j in i:\n",
    "            input_id.append(vocab2id[j])\n",
    "    #         break\n",
    "\n",
    "        if len(input_id) > ( max_seq_length - 2):\n",
    "            input_id = input_id[:max_seq_length - 2]\n",
    "        input_id.insert(0,vocab2id[\"[CLS]\"])   \n",
    "        while len(input_id) < (max_seq_length - 1):\n",
    "            input_id.append(0)\n",
    "        input_id.append(vocab2id[\"[SEP]\"])\n",
    "\n",
    "        input_ids.append(input_id)\n",
    "        \n",
    "    label_ids = []\n",
    "    for i in labels:\n",
    "        label_id = []\n",
    "    #     break\n",
    "        for j in i:\n",
    "            label_id.append(label2id[j])\n",
    "    #         break\n",
    "\n",
    "        if len(label_id) > ( max_seq_length - 2):\n",
    "            label_id = label_id[:max_seq_length - 2]\n",
    "        label_id.insert(0,label2id[\"[CLS]\"])   \n",
    "        while len(label_id) < (max_seq_length - 1):\n",
    "            label_id.append(0)\n",
    "        label_id.append(label2id[\"[SEP]\"])\n",
    "\n",
    "        label_ids.append(label_id)\n",
    "\n",
    "    return (words,labels,label_list,label2id,id2label,len(label_list),input_ids,label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(vocab,vocab2id,id2vocab,vocab_size)= readVocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "words,labels,label_list,label2id,id2label,label_size,input_ids,label_ids = handle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.Tensor(input_ids).long()\n",
    "label_ids = torch.Tensor(label_ids).long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自定义数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertNerDataset(tud.Dataset): \n",
    "    def __init__(self,input_ids,label_ids): \n",
    "        super(BertNerDataset,self).__init__() \n",
    "        self.label_ids = label_ids\n",
    "        self.input_ids = input_ids\n",
    "       \n",
    "    def __len__(self): \n",
    "        return len(self.input_ids) \n",
    "    def __getitem__(self, index): \n",
    "        return (self.input_ids[index,:],self.label_ids[index,:])\n",
    "\n",
    "dataset = BertNerDataset(input_ids[:8000],label_ids[:8000]) \n",
    "dataloader = tud.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True,num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlBertCRFNer(nn.Module):\n",
    "    '''\n",
    "    AlBert + CRF\n",
    "    '''\n",
    "    def __init__(self,vocab_size,config_file,num_tags=768):\n",
    "        \n",
    "        super(AlBertCRFNer,self).__init__()\n",
    "        config = AlbertConfig.from_pretrained(config_file)\n",
    "        self.albert = AlbertModel(config)\n",
    "        self.crf = CRF(num_tags)\n",
    "\n",
    "    def forward(self,x,tags=None):\n",
    "        y,_= self.albert(x) # 8,202,768\n",
    "        if tags is None:\n",
    "            output = y.permute(1,0,2)\n",
    "            return self.crf.decode(output) # 8 202\n",
    "        else:\n",
    "            return -self.crf(y,tags,reduction='mean') # (seq_length, batch_size, num_tags)   (seq_length, batch_size)--> (batch_size,).sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlBertBiLSTMCRFNer(nn.Module):\n",
    "    '''\n",
    "    AlBert + BiLSTM + CRF\n",
    "    '''\n",
    "    def __init__(self,vocab_size,config_file,num_tags=768):\n",
    "        super(AlBertBiLSTMCRFNer,self).__init__()\n",
    "        config = AlbertConfig.from_pretrained(config_file)\n",
    "        self.albert = AlbertModel(config)\n",
    "        self.bilstm = nn.LSTM(input_size = 768,hidden_size =768 //2, bidirectional =True)\n",
    "        self.crf = CRF(num_tags)\n",
    "        \n",
    "    def forward(self,x,tags=None):\n",
    "        y,_=  self.albert(x)\n",
    "        output,_ = self.bilstm(y) # 8 202 768\n",
    "        \n",
    "        if tags is None:\n",
    "            output = output.permute(1,0,2)\n",
    "            return self.crf.decode(output) # 8 202\n",
    "        else:\n",
    "            return -self.crf(output,tags,reduction='mean') # (seq_length, batch_size, num_tags)   (seq_length, batch_size)--> (batch_size,).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlBertBiGRUCRFNer(nn.Module):\n",
    "    '''\n",
    "    AlBert + BiGRU + CRF\n",
    "    '''\n",
    "    def __init__(self,vocab_size,config_file,num_tags=768):\n",
    "        super(AlBertBiGRUCRFNer,self).__init__()\n",
    "        config = AlbertConfig.from_pretrained(config_file)\n",
    "        self.albert = AlbertModel(config)\n",
    "        self.bigru = nn.GRU(input_size=768,hidden_size=768//2,bidirectional=True)\n",
    "        self.crf = CRF(num_tags)\n",
    "\n",
    "    def forward(self,x,tags=None):\n",
    "            \n",
    "        y,_ = self.albert(x)\n",
    "        output,_ = self.bigru(y)  \n",
    "        \n",
    "        if tags is None:\n",
    "            output = output.permute(1,0,2) \n",
    "            return self.crf.decode(output)\n",
    "        else:\n",
    "            return -self.crf(output,tags,reduction='mean')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlBertBiGRUCRFNer(\n",
       "  (albert): AlbertModel(\n",
       "    (embeddings): AlbertEmbeddings(\n",
       "      (word_embeddings): Embedding(30000, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "    (encoder): AlbertEncoder(\n",
       "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
       "      (transformer): AlbertTransformer(\n",
       "        (group): ModuleList(\n",
       "          (0): AlbertGroup(\n",
       "            (inner_group): ModuleList(\n",
       "              (0): AlbertLayer(\n",
       "                (attention): AlbertAttention(\n",
       "                  (self): AlbertSelfAttention(\n",
       "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (dropout): Dropout(p=0, inplace=False)\n",
       "                  )\n",
       "                  (output): AlbertSelfOutput(\n",
       "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (dropout): Dropout(p=0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (ffn): AlbertFFN(\n",
       "                  (intermediate): AlbertIntermediate(\n",
       "                    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                    (output): AlbertOutput(\n",
       "                      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                      (dropout): Dropout(p=0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (LayerNorm_1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): AlbertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (bigru): GRU(768, 384, bidirectional=True)\n",
       "  (crf): CRF(num_tags=768)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AlBertBiGRUCRFNer(vocab_size,config_file)\n",
    "model.load_state_dict(torch.load(\"outputs/AlBert-BiGRU-CRF.pt\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import recall_score,precision_score,f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_f1(true,pred):\n",
    "    m,n = true.shape\n",
    "    precision = np.zeros((m,))\n",
    "    recall = np.zeros((m,))\n",
    "    f1 = np.zeros((m,))\n",
    "    \n",
    "    for i in range(m):\n",
    "        precision[i] = precision_score(true[i],pred[i],average=\"macro\")\n",
    "        recall[i] = recall_score(true[i],pred[i],average=\"macro\")\n",
    "        f1[i] = f1_score(true[i],pred[i],average=\"macro\")\n",
    "        \n",
    "    return (precision.mean(),recall.mean(),f1.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.0007838283828382838 recall: 0.15833333333333333 f1: 0.0015599343185550081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\dsy\\desktop\\nlp_from_dsy\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "for step,(input_ids,label_ids) in enumerate(dataloader):\n",
    "    y_pred =  model(input_ids,None)\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_true = label_ids.numpy()\n",
    "    precision,recall,f1 = precision_recall_f1(y_true,y_pred) \n",
    "    print(\"precision:\",precision,\"recall:\",recall,\"f1:\",f1)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
